{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPlcgLVTuvzircII8NloIg/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chetnaarora93/Data-Analysis-Project/blob/main/Jamboree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jamboree has helped thousands of students like you make it to top colleges abroad. Be it GMAT, GRE or SAT, their unique problem-solving methods ensure maximum scores with minimum effort.\n",
        "They recently launched a feature where students/learners can come to their website and check their probability of getting into the IVY league college. This feature estimates the chances of graduate admission from an Indian perspective."
      ],
      "metadata": {
        "id": "VFj-d9GFZpM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTING LIBRARIES**"
      ],
      "metadata": {
        "id": "hFqaceXMZxMY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUNkYDufOY_M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **READING DATASET**"
      ],
      "metadata": {
        "id": "sZeiAfaPdmMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/001/839/original/Jamboree_Admission.csv\")\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "XPr6gTAGdsQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "QtWCVEDTeHDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset has 500 rows and 9 columns"
      ],
      "metadata": {
        "id": "qM_OvnVYeauG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "HHFMcgXteDG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no null values"
      ],
      "metadata": {
        "id": "IdOvWnymef6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CHECKING NULL VALUES**"
      ],
      "metadata": {
        "id": "xAw_c03br59Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "GkwEycrZgnt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "xKIG0XYOeLOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "yKMbm1eX4qw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no duplicate values"
      ],
      "metadata": {
        "id": "da__mhlu5Sxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NON GRAPHICAL ANALYSIS**"
      ],
      "metadata": {
        "id": "STQQJf4j6Dgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"University Rating\"].value_counts()"
      ],
      "metadata": {
        "id": "Krkn3v2K5YpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest number of universities are rated 3 (162 universities, 36.4%).\n",
        "The second most common rating is 2 (126 universities, 28.3%).\n",
        "Together, ratings 3 and 2 account for 64.7% of all universities, indicating that most universities are in the average-to-below-average range."
      ],
      "metadata": {
        "id": "Gtwz8waE_Szy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"SOP\"].value_counts()"
      ],
      "metadata": {
        "id": "K080AhvR6IBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df[\"LOR\"].value_counts()"
      ],
      "metadata": {
        "id": "pnFIW36X-Hs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Research\"].value_counts()"
      ],
      "metadata": {
        "id": "Lh7xK8_g-5TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UNIVARIATE ANALYSIS**"
      ],
      "metadata": {
        "id": "K2_00Qu3GIsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot histograms\n",
        "sns.histplot(df[\"GRE Score\"], bins=10, kde=True, ax=axes[0], color='blue')\n",
        "axes[0].set_title(\"GRE score Distribution\")\n",
        "\n",
        "sns.histplot(df[\"TOEFL Score\"], bins=10, kde=True, ax=axes[1], color='green')\n",
        "axes[1].set_title(\"TOEFL Score Distribution\")\n",
        "\n",
        "sns.histplot(df[\"CGPA\"], bins=10, kde=True, ax=axes[2], color='red')\n",
        "axes[2].set_title(\"CGPA Value Distribution\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5iVz6CSWIFeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRE Score Distribution: The histogram of GRE scores shows how the scores are spread among applicants. If the distribution is skewed, it may indicate that most students either score high or low.\n",
        "\n",
        "TOEFL Score Distribution: This plot helps understand the spread of TOEFL scores. A normal distribution suggests that most students score around the mean, while skewness might indicate a concentration of scores in a particular range.\n",
        "\n",
        "CGPA Distribution: The CGPA histogram provides insights into the academic performance of applicants. A right-skewed distribution could indicate that most applicants have high CGPAs, while a left-skewed distribution suggests lower GPAs.\n",
        "\n",
        "Comparison Across Metrics: If the distributions differ significantly, it may indicate variations in applicant profiles. For instance, a uniform distribution in GRE but a skewed CGPA distribution might suggest diverse academic backgrounds."
      ],
      "metadata": {
        "id": "Cg4RPWnfJ5kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIVARIATE ANALYSIS**"
      ],
      "metadata": {
        "id": "3ImkQGcK4VPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar plot between University Rating and Chance of Admit\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "sns.scatterplot(x=df[\"University Rating\"], y=df[\"Chance of Admit \"], color='purple', ax=axes[0])\n",
        "axes[0].set_title(\"University Rating vs Chance of Admit\")\n",
        "axes[0].set_xlabel(\"University Rating\")\n",
        "axes[0].set_ylabel(\"Chance of Admit\")\n",
        "\n",
        "# Bar plot between TOEFL Score and Chance of Admit\n",
        "sns.scatterplot(x=df[\"SOP\"], y=df[\"Chance of Admit \"], color='orange', ax=axes[1])\n",
        "axes[1].set_title(\"TOEFL Score vs Chance of Admit\")\n",
        "axes[1].set_xlabel(\"TOEFL Score\")\n",
        "axes[1].set_ylabel(\"Chance of Admit\")\n",
        "\n",
        "# Bar plot between CGPA and Chance of Admit\n",
        "sns.scatterplot(x=df[\"LOR \"], y=df[\"Chance of Admit \"], color='brown', ax=axes[2])\n",
        "axes[2].set_title(\"CGPA vs Chance of Admit\")\n",
        "axes[2].set_xlabel(\"CGPA\")\n",
        "axes[2].set_ylabel(\"Chance of Admit\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j-JkvMGSJ6y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MULTIVARIATE ANALYSIS**"
      ],
      "metadata": {
        "id": "O98FE0yXmsU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=\"University Rating\", y=\"Chance of Admit \", hue=\"Research\", data=df, palette=\"coolwarm\")\n",
        "plt.title(\"Chance of Admit vs University Rating (Grouped by Research Experience)\")\n",
        "plt.xlabel(\"University Rating\")\n",
        "plt.ylabel(\"Chance of Admit\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nDgvOO3CPTG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HMWZZO5gPSjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=\"University Rating\", y=\"Chance of Admit \", hue=\"Research\", data=df, palette=\"coolwarm\")\n",
        "plt.title(\"Chance of Admit vs University Rating (Grouped by Research Experience)\")\n",
        "plt.xlabel(\"University Rating\")\n",
        "plt.ylabel(\"Chance of Admit\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cNtc0viCH1tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df, vars=[\"GRE Score\", \"TOEFL Score\", \"CGPA\", \"Chance of Admit \"], hue=\"University Rating\", palette=\"coolwarm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7CmEeXGAlzeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dB5FQacGmlVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PREPROCESSING**"
      ],
      "metadata": {
        "id": "0Z-g3MVUm2BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "x=df.drop(\"Chance of Admit \",axis=1)\n",
        "y=df[\"Chance of Admit \"]"
      ],
      "metadata": {
        "id": "1G3ZDYuNpQ2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data for training and testing\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "vU_b7oKBrlUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NORMALIZING DATA**"
      ],
      "metadata": {
        "id": "MKzmE8j6tw-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler= MinMaxScaler()\n",
        "x_train_transform=scaler.fit_transform(x_train)\n",
        "x_test_transform=scaler.transform(x_test)\n",
        "x_train_transform_df = pd.DataFrame(x_train_transform)\n",
        "x_test_transform_df = pd.DataFrame(x_test_transform)\n",
        "\n",
        "# Display first 5 rows\n",
        "print('dependent_features_normalized_values', x_train_transform_df.head(5))\n",
        "print('independent_features_normalized_values', x_test_transform_df.head(5))"
      ],
      "metadata": {
        "id": "lH-2B8jLtB6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL BUILDING**"
      ],
      "metadata": {
        "id": "NMcBB2u3w_kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr=LinearRegression()\n",
        "lr.fit(x_train_transform,y_train)"
      ],
      "metadata": {
        "id": "LpRViSGrw_LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predict_test=lr.predict(x_test_transform)\n",
        "predict_train=lr.predict(x_train_transform)\n"
      ],
      "metadata": {
        "id": "tCFkYZJmq8lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINDING ERRORS**"
      ],
      "metadata": {
        "id": "WjXeGKzPsYC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#caluclating errors of test data\n",
        "MSE= mean_squared_error(y_test,predict_test)\n",
        "RMSE=np.sqrt(MSE)\n",
        "MAE=mean_absolute_error(y_test,predict_test)\n",
        "r2_score= r2_score(y_test,predict_test)\n",
        "adjusted_r2_score = 1 - (1 - r2_score) * (len(y_test) - 1) / (len(y_test) - x_test.shape[1] - 1)\n",
        "print(\"MSE:\",MSE)\n",
        "print(\"RMSE:\",RMSE)\n",
        "print(\"MAE:\",MAE)\n",
        "print(\"r2_score:\",r2_score)\n",
        "print(\"adjusted_r2_score:\",adjusted_r2_score)"
      ],
      "metadata": {
        "id": "UjikMtT4vFdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#caluclating errors of trained data\n",
        "MSE=mean_squared_error(y_train,predict_train)\n",
        "RMSE=np.sqrt(MSE)\n",
        "MAE=mean_absolute_error(y_train,predict_train)\n",
        "# r2_train = r2_score(y_train, predict_train)\n",
        "adjusted_r2_score = 1 - (1 - r2_score) * (len(y_train) - 1) / (len(y_train) - x_train.shape[1] - 1)\n",
        "print(\"MSE:\",MSE)\n",
        "print(\"RMSE:\",RMSE)\n",
        "print(\"MAE:\",MAE)\n",
        "print(\"r2_score:\",r2_score)\n",
        "print(\"adjusted_r2_score:\",adjusted_r2_score)"
      ],
      "metadata": {
        "id": "-KhRQlV9WK5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Model Accuracy and Fit\n",
        "Test R² Score: 0.8263 (82.63%)\n",
        "Train R² Score: (Needs recalculating, but likely similar to test R²)\n",
        "This means that 82.63% of the variance in the target variable is explained by your model, which is a strong fit.\n",
        "If the train R² is close to the test R², it suggests the model generalizes well and isn't overfitting.\n",
        "2. Model Errors & Stability\n",
        "Test MSE: 0.00355, Test RMSE: 0.0596, Test MAE: 0.0433\n",
        "Train MSE: 0.00338, Train RMSE: 0.0581, Train MAE: 0.0422\n",
        "The train and test errors are very close, which indicates low variance and suggests that the model is not overfitting or underfitting.\n",
        "RMSE and MAE are low, meaning the average prediction error is quite small.\n",
        "3. Overfitting or Underfitting Check\n",
        "The train and test errors are almost the same, which means:\n",
        "The model is not overfitting (if it were, train errors would be much lower than test errors).\n",
        "The model is not underfitting (if it were, both train and test R² would be significantly lower).\n",
        "This balance suggests a well-tuned model with good generalization.\n",
        "4. Adjusted R² Interpretation\n",
        "Test Adjusted R²: 0.8111\n",
        "Train Adjusted R²: To be recalculated\n",
        "Since Adjusted R² is close to R², it means that adding extra features did not add unnecessary complexity.\n",
        "If Adjusted R² was significantly lower than R², it would indicate that some features are not contributing to the model."
      ],
      "metadata": {
        "id": "Gj9P9shZdEP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ The model is performing well with strong predictive power.\n",
        "✅ No signs of overfitting or underfitting.\n",
        "✅ Errors are low, and the variance between test and train data is minimal.\n",
        "✅ Adjusted R² confirms that features are meaningful."
      ],
      "metadata": {
        "id": "ixqxgo-IdJYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# O**rdinary Least Squares**\n",
        "\n",
        "Ordinary Least Squares (OLS) is a method used to estimate the coefficients (parameters) of a linear regression model by minimizing the sum of squared residuals (errors). It finds the best-fitting line that minimizes the difference between the observed values and the predicted values."
      ],
      "metadata": {
        "id": "sSbQ6LiIFS5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add an intercept to the features\n",
        "X_sm = sm.add_constant(x_train_transform)\n",
        "\n",
        "# Fit OLS model\n",
        "model = sm.OLS(y_train, X_sm)\n",
        "results = model.fit()\n",
        "\n",
        "# Print model summary\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "BDnXqtPGdK3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LASSO AND RIDGE RIDGE REGRESSION**"
      ],
      "metadata": {
        "id": "94G9k7yF2Uhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelm1=Lasso(alpha=0.001)\n",
        "modelm1.fit(x_train_transform,y_train)\n",
        "\n",
        "modelm2=Ridge(alpha=0.001)\n",
        "modelm2.fit(x_train_transform,y_train)\n"
      ],
      "metadata": {
        "id": "eK-eVXm12hl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1_predict_test=modelm1.predict(x_test_transform)\n",
        "x2_predict_test=modelm2.predict(x_test_transform)\n",
        "\n",
        "x1_predict_train=modelm1.predict(x_train_transform)\n",
        "x2_predict_train=modelm2.predict(x_train_transform)\n"
      ],
      "metadata": {
        "id": "5OkiBGrM2xTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE_lasso = mean_squared_error(y_test, x1_predict_test)\n",
        "RMSE_lasso = np.sqrt(MSE_lasso)  # RMSE Calculation\n",
        "MAE_lasso = mean_absolute_error(y_test, x1_predict_test)\n",
        "\n",
        "print(\"Lasso Regression - Test Data:\")\n",
        "print(\"MSE:\", MSE_lasso)\n",
        "print(\"RMSE:\", RMSE_lasso)\n",
        "print(\"MAE:\", MAE_lasso)\n",
        "\n",
        "\n",
        "# Ridge Regression Evaluation\n",
        "MSE_ridge = mean_squared_error(y_test, x2_predict_test)\n",
        "RMSE_ridge = np.sqrt(MSE_ridge)\n",
        "MAE_ridge = mean_absolute_error(y_test, x2_predict_test)\n",
        "\n",
        "\n",
        "print(\"\\nRidge Regression - Test Data:\")\n",
        "print(\"MSE:\", MSE_ridge)\n",
        "print(\"RMSE:\", RMSE_ridge)\n",
        "print(\"MAE:\", MAE_ridge)\n",
        "\n",
        "\n",
        "MSE_lasso = mean_squared_error(y_train, x1_predict_train)\n",
        "RMSE_lasso = np.sqrt(MSE_lasso)  # RMSE Calculation\n",
        "MAE_lasso = mean_absolute_error(y_train, x1_predict_train)\n",
        "\n",
        "print(\"\\nLasso Regression - train Data:\")\n",
        "print(\"MSE:\", MSE_lasso)\n",
        "print(\"RMSE:\", RMSE_lasso)\n",
        "print(\"MAE:\", MAE_lasso)\n",
        "\n",
        "\n",
        "# Ridge Regression Evaluation\n",
        "MSE_ridge = mean_squared_error(y_train, x2_predict_train)\n",
        "RMSE_ridge = np.sqrt(MSE_ridge)\n",
        "MAE_ridge = mean_absolute_error(y_train, x2_predict_train)\n",
        "\n",
        "\n",
        "print(\"\\nRidge Regression - train Data:\")\n",
        "print(\"MSE:\", MSE_ridge)\n",
        "print(\"RMSE:\", RMSE_ridge)\n",
        "print(\"MAE:\", MAE_ridge)"
      ],
      "metadata": {
        "id": "Bm8-KZk53xFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both models perform well with low errors and no clear overfitting.\n",
        "Lasso slightly outperforms Ridge in terms of lower error metrics.\n",
        "If feature selection is important, Lasso is preferable.\n",
        "If all features are expected to be important, Ridge is more stable."
      ],
      "metadata": {
        "id": "XbTb1K9cXogm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate errors\n",
        "lasso_errors = y_test - x1_predict_test\n",
        "ridge_errors = y_test - x2_predict_test\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(lasso_errors, bins=30, kde=True, color=\"blue\")\n",
        "plt.xlabel(\"Error\")\n",
        "plt.title(\"Lasso Regression Error Distribution\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(ridge_errors, bins=30, kde=True, color=\"red\")\n",
        "plt.xlabel(\"Error\")\n",
        "plt.title(\"Ridge Regression Error Distribution\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z1KyDufO3fAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ASSUMPTIONS OF LINEAR REGRESSION**"
      ],
      "metadata": {
        "id": "j-LP-ZLDlwJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NORMALITY OF RESIDUAL CHECK**\n",
        "\n",
        "The normality of residuals in regression analysis refers to the assumption that the residuals (the differences between observed and predicted values) follow a normal distribution.\n",
        "\n",
        "Violations of this assumption can lead to incorrect conclusions about the significance of predictor variables and the overall fit of the model.\n",
        "\n",
        "Residuals = y-y_hat\n",
        "\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "y = w0 + wTx +(y-y_hat)\n"
      ],
      "metadata": {
        "id": "1RNuNs8077il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding Training and Validation Residuals\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# residuals of training data\n",
        "X_train_scaled = scaler.fit_transform(x_train)\n",
        "X_sm = sm.add_constant(X_train_scaled)\n",
        "sm_model = sm.OLS(y_train, X_sm).fit()\n",
        "Y_predict_tr = sm_model.predict(X_sm)\n",
        "errors_train = Y_predict_tr - y_train\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training data\n",
        "# X_train_sm = sm.add_constant(X_train_scaled)\n",
        "# sm_model = sm.OLS(y_train, X_train_sm).fit()\n",
        "# Y_hat_tr = sm_model.predict(X_train_sm)\n",
        "\n",
        "# residuals of validation data\n",
        "X_test_scaled = scaler.fit_transform(x_test)\n",
        "X_sm = sm.add_constant(X_test_scaled)\n",
        "sm_model = sm.OLS(y_test, X_sm).fit()\n",
        "Y_predict_ts = sm_model.predict(X_sm)\n",
        "errors_test = Y_predict_ts - y_test"
      ],
      "metadata": {
        "id": "qt5kt7nvl9a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# histogram for training and validation errors\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
        "\n",
        "sns.histplot(errors_train, kde= True, ax= axes[0], color=\"r\")\n",
        "axes[0].set_xlabel(\"Training Residuals\")\n",
        "axes[0].set_title(\"Histogram of training residuals\")\n",
        "\n",
        "sns.histplot(errors_test, kde= True, ax= axes[1])\n",
        "axes[1].set_xlabel(\"Validation Residuals\")\n",
        "axes[1].set_title(\"Histogram of validation residuals\")\n"
      ],
      "metadata": {
        "id": "ghh5Bg2mLAkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution seems to be normal."
      ],
      "metadata": {
        "id": "XH06kBqXM8x7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Of Residuals**"
      ],
      "metadata": {
        "id": "SOL4l8hxNAkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errors_train.mean()\n",
        "errors_test.mean()\n",
        "print(\"Mean of Training Residuals:\", errors_train.mean())\n",
        "print(\"Mean of Validation Residuals:\", errors_test.mean())"
      ],
      "metadata": {
        "id": "ZKntCNBNNJDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean of residuals of both training and validation data is very close to 0."
      ],
      "metadata": {
        "id": "60U4TD50NVxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HOMOSCEDESTICITY**\n",
        "\n",
        "Homoscedasticity, also known as homogeneity of variance, is a key assumption in regression analysis. It refers to the condition where the variance of the residuals is constant across all levels of the independent variables. It means that the spread of the residuals remains the same as you move along the range of the predictor variables.\n",
        "\n",
        "Homoscedasticity ensures that the standard errors of the regression coefficients are estimated accurately.\n",
        "\n",
        "In a homoscedastic model, the variability of the residuals is consistent across the range of the predictor variables. This consistency allows for more reliable predictions from the regression model."
      ],
      "metadata": {
        "id": "nWw5GQmCSF7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scatterplot for Homoscedasticity check for training and validation data\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "sns.scatterplot(x=Y_predict_tr, y=errors_train, ax=axes[0])\n",
        "axes[0].set_xlabel(\"Predicted Training Selling price\")\n",
        "axes[0].set_ylabel(\"Training Residuals\")\n",
        "axes[0].set_title(\"Predicted values vs Training Residuals\")\n",
        "\n",
        "\n",
        "sns.scatterplot(x=Y_predict_ts, y=errors_test, ax=axes[1])\n",
        "axes[1].set_xlabel(\"Predicted Validation Selling price\")\n",
        "axes[1].set_ylabel(\"Validation Residuals\")\n",
        "axes[1].set_title(\"Predicted values vs Validation Residuals\")\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Efp3mQrwSKly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hVNXXphJP6xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the residuals are roughly even across all the predicted values, we can say that the property of Homoscedasticity is met.\n",
        "\n"
      ],
      "metadata": {
        "id": "c6ej17oNTULr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wBoGvVV5P-O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_vif(calculate):\n",
        "    vif = pd.DataFrame()\n",
        "    vif['Features'] = x_train.columns\n",
        "    vif['VIF'] = [variance_inflation_factor(calculate.values, i) for i in range(calculate.shape[1])]\n",
        "    return vif"
      ],
      "metadata": {
        "id": "XMTL4tbvP_CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_vif(x_train_transform_df)"
      ],
      "metadata": {
        "id": "8As7ywqJwukI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=x_train.drop([\"GRE Score\", \"CGPA\", \"LOR \"], axis=1)\n",
        "calculate_vif(x_train)\n"
      ],
      "metadata": {
        "id": "sevM9KwqS7sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=x_train.drop([\"TOEFL Score\",\"SOP\"], axis=1)\n",
        "calculate_vif(x_train)"
      ],
      "metadata": {
        "id": "rwpTivIAmLqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serial No, University rating and research had Vif less than 5, so we can consifer them for feature evaluation"
      ],
      "metadata": {
        "id": "2alw7MaUmlNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ASSUMPTION OF LINEARITY**\n",
        "\n",
        "The assumption of linearity in linear regression refers to the relationship between the independent variables (predictors) and the dependent variable (response).\n",
        "\n",
        "It assumes that the relationship between the predictors and the response variable is linear or can be adequately approximated by a linear function."
      ],
      "metadata": {
        "id": "AebJt18ureQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# H0 : There is no linear correlation between the two variables\n",
        "# Ha: There is linear correlation between the two variables\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "alpha=0.05\n",
        "column = x_train.columns\n",
        "for c in column:\n",
        "\n",
        "    pstat,pval = pearsonr(x_train[c],y_train)\n",
        "    print('Column Name:',c)\n",
        "    if pval < alpha:\n",
        "        print('P-value:',pval)\n",
        "        print('There is a linear correlation between the independent and dependent variable')\n",
        "    else:\n",
        "        print('There is no linear correlation between the independent and dependent variable')\n",
        "    print('\\n')\n",
        "    print('-'*100)"
      ],
      "metadata": {
        "id": "7I972Omsqw6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INSIGHTS**"
      ],
      "metadata": {
        "id": "8olxq8njrj46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The target variable Chance of Admit, is mostly dependent on many features like CGPA, GRE Score, TOEFL Score and SOP-LOR.\n",
        "\n",
        "2. The factor which mostly influences the Admit chance is the CGPA.\n",
        "\n",
        "3. Other factors such as Research and University ratings are also there but they are not contributing much to the Chance of Admit.\n",
        "\n",
        "4. Also, there are certain students who have done research and have high university ratings they have a high Chance of Admit.\n",
        "\n",
        "5. Students which have great Chance of Admit have a CGPA around or above 8.5, and also a good GRE Score.\n",
        "\n",
        "6. The features are concentrated within a specified range, they dont have any outlier data points.\n",
        "\n",
        "7. During model creation, the best score by ridge and lasso was almost same and was around 81% on training data and 83% on validation data."
      ],
      "metadata": {
        "id": "TF1bsBlqrkif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RECOMMENDATION**"
      ],
      "metadata": {
        "id": "qMCXk-PKrpKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Focusing on Key Features: Given that the Chance of Admit is mostly dependent on features like CGPA, GRE Score, TOEFL Score, and SOP-LOR, it's important to focus the analysis on these key features. These features should be prioritized during feature selection and model building.\n",
        "\n",
        "2. Emphasize CGPA: Since CGPA is identified as the most influential factor in determining the Chance of Admit, special attention should be given to this feature during model building and analysis.\n",
        "\n",
        "3. Considering Research and University Ratings: Although Research and University ratings may not contribute significantly to the Chance of Admit individually, they still play a role in certain cases. It's important to consider them as additional factors in the analysis, especially for students with high university ratings and research experience.\n",
        "\n",
        "4. Exploring Feature Interactions: Investigating potential interactions between features, especially for students with high Chance of Admit. It may be beneficial to examine how different combinations of features contribute to the overall likelihood of admission.\n",
        "\n",
        "5. Segmentation Analysis: Conducting segmentation analysis to identify groups of students with similar characteristics and Chance of Admit. This can help tailoring admission strategies and identifying specific target groups for recruitment efforts.\n",
        "\n",
        "6. Ensure Model Robustness: Since the Ridge and Lasso models produced almost similar scores, it's essential to ensure the robustness of the models and validate their performance on independent datasets.\n",
        "\n",
        "7. Continuous Monitoring and Improvement: Continuously monitoring the model performance by updating the model with new data and refining the feature selection process can help maintain its effectiveness over time."
      ],
      "metadata": {
        "id": "9ctMwhlorsh9"
      }
    }
  ]
}